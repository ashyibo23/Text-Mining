# -*- coding: utf-8 -*-
"""text_mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jjq6GKRqbNnbQiewa8Pu8FdM0HpEBe4Y
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from sklearn import metrics

df = pd.read_csv('/content/Corona_NLP_train.csv', encoding='latin1')
df.head()

df.loc[[0, 1]]

df.iloc[0:2]

df.loc[0]

df.info()

df['Sentiment']

#computing the possible sentiment values
possible_sentiment = df.Sentiment.unique()
print("The possible sentiments: ", str (possible_sentiment))

sentiment_counts = df.Sentiment.value_counts()
print(sentiment_counts)

second_popular_sentiment = list(sentiment_counts.items())[1][0]
print('The most popular sentiment:', second_popular_sentiment)

plt.figure(figsize=(9,4))
sns.set_theme(style="darkgrid")
sns.countplot(x = 'Sentiment', hue='Sentiment', data= df)
plt.show()

#date with the greatest number of extremely positive tweets.

extremely_positive_tweets = df[df["Sentiment"] == "Extremely Positive"]
tweets1 = list(extremely_positive_tweets['TweetAt'].value_counts().items())[0][0]
print('Date with the greatest amount of tweets of extremely positive on:', tweets1 )

#same as above
tweets2 = extremely_positive_tweets['TweetAt'].value_counts().index.tolist()[0]
print('Date with the greatest amount of tweets of extremely positive on:', tweets2 )

df['OriginalTweet']

non_alphabetical = df['OriginalTweet'].str.replace('[^a-zA-Z]', ' ', regex=True)
#spaces = non_alphabetical.str.strip()
print(non_alphabetical)

#non-alphabetical characters with whitespaces
non_alphabetical = df['OriginalTweet'].str.replace('[^a-zA-Z]', ' ', regex=True)
spaces = non_alphabetical.str.replace('\s+', ' ', regex=True)
print(spaces)

lower_case = spaces.str.lower()
print(lower_case)

#Tokenize the tweets
tokenize_tweets = lower_case.str.split(expand = True).stack().value_counts()
print(tokenize_tweets)

#count the total number of all words and distinct 
total = sum(tokenize_tweets)
print("The total number of all words:", total)
print("Total no. of distinct words:", len(set(tokenize_tweets)))

#the 10 most frequent words in the corpus.
print("The top 10 most frequent words:")
print(tokenize_tweets[:10])

#Remove stop words, words with <= 2 characters
stop_words = lower_case.str.findall('\w{3,}').str.join(' ')
print(stop_words)

#Tokenize
stop_words_tweets = stop_words.str.split(expand = True).stack().value_counts()
#recounting the total number of words
total_sum = sum(stop_words_tweets)
print("The total sum of all words:", total_sum)


#the 10 most frequent words in the modified corpus

print("The top 10 most frequent words:")
print(stop_words_tweets[:10])

#Multinomial Naive Bayes classiifer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

corpus = np.array(df["OriginalTweet"])
target = np.array(df.Sentiment)
vectorizer = CountVectorizer()

X = vectorizer.fit_transform(corpus)

clf = MultinomialNB()
clf.fit(X, target)

#class predications
class_pred = clf.predict(X)
print("Actual")
print(df['Sentiment'][:10])
print("predicted")
print(class_pred[:10])

#calculating accuracy 
#model.score(target, class_pred)
accuracy = metrics.accuracy_score(target, class_pred)
error_rate = str(round((1 - accuracy) * 100, 2)) + "%"
print("Multinomial NB classifier error rate: " +  error_rate)
print("Accuracy: ", accuracy)